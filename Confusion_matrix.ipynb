{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8578f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# model_name = \"kaggle_trashbox_10epoch_flip_only\"\n",
    "# model = tf.keras.models.load_model('./trained_model/'+ model_name +\".h5\")\n",
    "# # model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c2ef3",
   "metadata": {},
   "source": [
    "## Create tf.data.dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d633fdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 409 files belonging to 3 classes.\n",
      "['Alu', 'Glass', 'PET']\n",
      "Found 412 files belonging to 3 classes.\n",
      "Found 1840 files belonging to 3 classes.\n",
      "Found 9200 files belonging to 3 classes.\n",
      "Found 490 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_trashbox_val_path = \"E:/data/Trashbox/trashbox_val/\"\n",
    "test_trashbox_test_path = \"E:/data/Trashbox/trashbox_test/\"\n",
    "test_recycling_path = \"E:/data/Recycling/Recycling_test/\"\n",
    "test_recycling_9k_path = \"E:/data/Recycling/Recycling_dataset_224_3/\"\n",
    "test_img_real_3_path = \"E:/data/Img_real_3/\"\n",
    "test_trashnet_path = \"E:/data/TrashNet/\"\n",
    "test_vending_5class_path = \"E:/data/UEH_vending/UEH_vending_5class/UEH_vending_5class_test/\"\n",
    "\n",
    "# test_vending_5class = tf.keras.utils.image_dataset_from_directory(\n",
    "#   test_vending_5class_path,  \n",
    "#   image_size=(224, 224),\n",
    "#   label_mode = 'categorical', \n",
    "#   shuffle=False,\n",
    "#   batch_size=32)\n",
    "\n",
    "# normalization_layer = layers.Rescaling(1./255)\n",
    "batch_size = 64\n",
    "#1\n",
    "test_trashbox_val = tf.keras.utils.image_dataset_from_directory(\n",
    "  test_trashbox_val_path,  \n",
    "  image_size=(224, 224),\n",
    "  label_mode = 'categorical', \n",
    "  shuffle=False,\n",
    "  batch_size=batch_size)\n",
    "\n",
    "class_names = test_trashbox_val.class_names\n",
    "print(class_names)\n",
    "\n",
    "# test_trashbox_val = test_trashbox_val.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "#2\n",
    "test_trashbox_test = tf.keras.utils.image_dataset_from_directory(\n",
    "  test_trashbox_test_path,  \n",
    "  image_size=(224, 224),\n",
    "  label_mode = 'categorical', \n",
    "  shuffle=False,\n",
    "  batch_size=batch_size)\n",
    "\n",
    "# test_trashbox_test = test_trashbox_test.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "#3\n",
    "test_recycling = tf.keras.utils.image_dataset_from_directory(\n",
    "  test_recycling_path,  \n",
    "  image_size=(224, 224),\n",
    "  label_mode = 'categorical', \n",
    "  shuffle=False,\n",
    "  batch_size=batch_size)\n",
    "\n",
    "# test_recycling = test_recycling.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "#4\n",
    "test_recycling_9k = tf.keras.utils.image_dataset_from_directory(\n",
    "  test_recycling_9k_path,  \n",
    "  image_size=(224, 224),\n",
    "  label_mode = 'categorical', \n",
    "  shuffle=False,\n",
    "  batch_size=batch_size)\n",
    "\n",
    "# test_recycling_9k = test_recycling_9k.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "#5\n",
    "test_img_real_3 = tf.keras.utils.image_dataset_from_directory(\n",
    "  test_img_real_3_path,  \n",
    "  image_size=(224, 224),\n",
    "  label_mode = 'categorical', \n",
    "  shuffle=False,\n",
    "  batch_size=batch_size)\n",
    "\n",
    "# test_img_real_3 = test_img_real_3.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# test_trashnet = tf.keras.utils.image_dataset_from_directory(\n",
    "#   test_trashnet_path,  \n",
    "#   image_size=(224, 224),\n",
    "#   label_mode = 'categorical', \n",
    "#   shuffle=False,\n",
    "#   batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b520fa",
   "metadata": {},
   "source": [
    "## Evaluate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a83811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def Create_cm(predictions, labels_list, title, test_ds, class_names, cm = False, normalize = None, save = False):\n",
    "\n",
    "    if cm:\n",
    "        arr = confusion_matrix(labels_list, predictions, normalize = normalize)   \n",
    "\n",
    "        df_cm = pd.DataFrame(arr, index = [i for i in class_names],\n",
    "                          columns = [i for i in class_names])\n",
    "        plt.figure(figsize=(8,6), dpi=100) # dpi for better resolution\n",
    "\n",
    "        # Scale up the size of all text\n",
    "        sn.set(font_scale = 1.1)\n",
    "\n",
    "        # plt.ticklabel_format(useOffset=False,style='plain', axis='y')\n",
    "        \n",
    "        \n",
    "        if normalize == \"true\":\n",
    "            ax = sn.heatmap(df_cm, annot=True, fmt=\".3f\", cmap='Blues')\n",
    "        else:\n",
    "            ax = sn.heatmap(df_cm, annot=True, fmt=\"d\", cmap='Blues')    \n",
    "        ax.set_title(title, fontsize=14, pad=20)\n",
    "        plt.ylabel('Actual label')\n",
    "        ax.set_ylabel(\"Actual label\", fontsize=14, labelpad=20)\n",
    "\n",
    "        plt.xlabel('Predicted label')\n",
    "        ax.set_xlabel('Predicted label',fontsize=14, labelpad=20)\n",
    "        print(classification_report(labels_list, predictions, digits=3))\n",
    "    if save:\n",
    "        fig = ax.get_figure()\n",
    "        fig.savefig(model_name + title + \"_3class.jpg\")\n",
    "    \n",
    "\n",
    "def Evaluate(model, test_ds, title, class_names, cm = False, normalize = None, save = False):\n",
    "\n",
    "    print(title)\n",
    "    \n",
    "    # Predictions are from model.predict, true labels are from test_ds\n",
    "    result = model.predict(test_ds, verbose = False)\n",
    "    \n",
    "    # Predictions\n",
    "    num_images, num_classes = result.shape\n",
    "    predictions = []\n",
    "    for i in range(num_images):\n",
    "        predictions.append(np.argmax(result[i]))\n",
    "\n",
    "    # True labels    \n",
    "    labels =  np.array([])\n",
    "    label = []\n",
    "    for _, y in test_ds:\n",
    "        labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])\n",
    "\n",
    "    labels_list = labels.astype('uint8').tolist()\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(labels_list, predictions)\n",
    "    f1 = f1_score(labels_list, predictions, average=\"macro\")\n",
    "    print(\"{:.3f} {:.3f}\".format(acc, f1))\n",
    "    \n",
    "    Create_cm(predictions, labels_list, title, test_ds, class_names, cm, normalize, save) \n",
    "    \n",
    "    return (acc, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b5760",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab53bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  model_Inc_Contrast\n",
      "Trashbox test\n",
      "0.905 0.878\n",
      "Trashbox val\n",
      "0.912 0.884\n",
      "Recycling 9k\n",
      "0.838 0.808\n",
      "Test Img Real\n",
      "0.782 0.754\n",
      "\n",
      "0.912 0.884\n",
      "0.905 0.878\n",
      "0.838 0.808\n",
      "0.782 0.754\n",
      "acc: 0.859\n",
      "f1: 0.831\n",
      "avg: 0.845\n",
      "Model:  model_Inc_Rot\n",
      "Trashbox test\n",
      "0.898 0.860\n",
      "Trashbox val\n",
      "0.900 0.867\n",
      "Recycling 9k\n",
      "0.834 0.804\n",
      "Test Img Real\n",
      "0.739 0.720\n",
      "\n",
      "0.900 0.867\n",
      "0.898 0.860\n",
      "0.834 0.804\n",
      "0.739 0.720\n",
      "acc: 0.843\n",
      "f1: 0.813\n",
      "avg: 0.828\n",
      "Model:  model_Inc_Crop\n",
      "Trashbox test\n",
      "0.908 0.877\n",
      "Trashbox val\n",
      "0.905 0.874\n",
      "Recycling 9k\n",
      "0.828 0.809\n",
      "Test Img Real\n",
      "0.731 0.712\n",
      "\n",
      "0.905 0.874\n",
      "0.908 0.877\n",
      "0.828 0.809\n",
      "0.731 0.712\n",
      "acc: 0.843\n",
      "f1: 0.818\n",
      "avg: 0.830\n",
      "Model:  model_Inc_flip_hor\n",
      "Trashbox test\n",
      "0.905 0.876\n",
      "Trashbox val\n",
      "0.910 0.883\n",
      "Recycling 9k\n",
      "0.854 0.829\n",
      "Test Img Real\n",
      "0.782 0.756\n",
      "\n",
      "0.910 0.883\n",
      "0.905 0.876\n",
      "0.854 0.829\n",
      "0.782 0.756\n",
      "acc: 0.863\n",
      "f1: 0.836\n",
      "avg: 0.849\n",
      "Model:  model_Inc_flip_vert\n",
      "Trashbox test\n",
      "0.898 0.856\n",
      "Trashbox val\n",
      "0.905 0.873\n",
      "Recycling 9k\n",
      "0.838 0.812\n",
      "Test Img Real\n",
      "0.763 0.741\n",
      "\n",
      "0.905 0.873\n",
      "0.898 0.856\n",
      "0.838 0.812\n",
      "0.763 0.741\n",
      "acc: 0.851\n",
      "f1: 0.821\n",
      "avg: 0.836\n"
     ]
    }
   ],
   "source": [
    "# test_trashbox_val, test_trashbox_test, test_recycling_9k, test_img_real_3\n",
    "# def Evaluate(model, test_ds, title, cm = False, normalize = None, save = False):\n",
    "# import tensorflow as tf\n",
    "\n",
    "# test_img_real_3_path = \"E:/data/UEH_7class/Val/\"\n",
    "\n",
    "# test_img_real_3 = tf.keras.utils.image_dataset_from_directory(\n",
    "#   test_img_real_3_path,  \n",
    "#   image_size=(224, 224),\n",
    "#   label_mode = 'categorical', \n",
    "#   shuffle=False,\n",
    "#   batch_size=32)\n",
    "\n",
    "model_list = [\n",
    "    \"model_Inc_Contrast\",\n",
    "    \"model_Inc_Rot\",\n",
    "    \"model_Inc_Crop\", \n",
    "    \"model_Inc_flip_hor\",\n",
    "    \"model_Inc_flip_vert\"\n",
    "]\n",
    "\n",
    "for model_name in model_list: \n",
    "    print(\"Model: \", model_name)\n",
    "    model = tf.keras.models.load_model('./trained_model/'+ model_name +\".h5\")\n",
    "    \n",
    "    # acc_4, f1_4 = Evaluate(model, test_vending_5class, \"Test\", cm = True, normalize = None, save = True)\n",
    "    # acc_1, f1_1 = Evaluate(model, test_img_real_3, \"Img Real\", class_names, cm = True, normalize = \"true\")\n",
    "    acc_1, f1_1 = Evaluate(model, test_trashbox_test, \"Trashbox test\", class_names,cm = False, normalize = \"true\")\n",
    "    acc_2, f1_2 = Evaluate(model, test_trashbox_val, \"Trashbox val\", class_names,cm = False, normalize = \"true\")\n",
    "    acc_3, f1_3 = Evaluate(model, test_recycling_9k, \"Recycling 9k\",class_names, cm = False, normalize = \"true\")\n",
    "    acc_4, f1_4 = Evaluate(model, test_img_real_3, \"Test Img Real\", class_names,cm = False, normalize = \"true\")\n",
    "\n",
    "    print()\n",
    "    print(\"{:.3f}\".format(acc_2), \"{:.3f}\".format(f1_2))\n",
    "    print(\"{:.3f}\".format(acc_1), \"{:.3f}\".format(f1_1))\n",
    "    print(\"{:.3f}\".format(acc_3), \"{:.3f}\".format(f1_3))\n",
    "    print(\"{:.3f}\".format(acc_4), \"{:.3f}\".format(f1_4))\n",
    "    # print(acc_2, f1_2)\n",
    "    # print(acc_3, f1_3)\n",
    "    # print(acc_4, f1_4)\n",
    "\n",
    "    \n",
    "    avg_acc = (acc_1 + acc_2 + acc_3 + acc_4)/4\n",
    "    avg_f1 = (f1_1 + f1_2 + f1_3 + f1_4)/4\n",
    "    avg = (avg_acc + avg_f1)/2\n",
    "\n",
    "    print(\"acc: {:.3f}\".format(avg_acc))\n",
    "    print(\"f1: {:.3f}\".format(avg_f1))\n",
    "    print(\"avg: {:.3f}\".format(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c2f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"kaggle_trashbox_30epoch_flip_oneof_05\"\n",
    "model = tf.keras.models.load_model('./trained_model/'+ model_name +\".h5\")\n",
    "# model.summary()\n",
    "\n",
    "# acc_3, f1_3 = Evaluate(model, test_recycling_9k, \"Recycling 9k\", cm = True, normalize = \"true\")\n",
    "acc_4, f1_4 = Evaluate(model, test_trashbox_val, \"TrashBox Val\", cm = True, normalize = None, save = True)\n",
    "# acc_5, f1_5 = Evaluate(model, test_recycling, \"RECYCLE\", cm = True, normalize = None, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_acc = (acc_1 + acc_2 + acc_3 + acc_4)/4\n",
    "avg_f1 = (f1_1 + f1_2 + f1_3 + f1_4)/4\n",
    "avg = (avg_acc + avg_f1)/2\n",
    "\n",
    "print(\"acc: {:.3f}\".format(avg_acc))\n",
    "print(\"f1: {:.3f}\".format(avg_f1))\n",
    "print(\"avg: {:.3f}\".format(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"True_augment_flip_hor_nas_3class_20epoch\"\n",
    "model = tf.keras.models.load_model('./trained_model/'+ model_name +\".h5\")\n",
    "#1\n",
    "[_, acc, precision, recall] = model.evaluate(test_trashbox_val, verbose = False)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(\"{:.3f} {:.3f}\".format(acc, f1))\n",
    "#2\n",
    "[_, acc, precision, recall] = model.evaluate(test_trashbox_test, verbose = False)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(\"{:.3f} {:.3f}\".format(acc, f1))\n",
    "#3\n",
    "# [_, acc, precision, recall] = model.evaluate(test_recycling, verbose = False)\n",
    "# f1 = 2*precision*recall/(precision+recall)\n",
    "# print(\"{:.3f} {:.3f}\".format(acc, f1))\n",
    "#4\n",
    "[_, acc, precision, recall] = model.evaluate(test_recycling_9k, verbose = False)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(\"{:.3f} {:.3f}\".format(acc, f1))\n",
    "#5\n",
    "[_, acc, precision, recall] = model.evaluate(test_img_real_3, verbose = False)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(\"{:.3f} {:.3f}\".format(acc, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f04d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[_, acc, precision, recall] = model.evaluate(test_trashbox_val, verbose = False)\n",
    "print([_, acc, precision, recall])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9325c3f",
   "metadata": {},
   "source": [
    "# QUICK TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb11e260",
   "metadata": {},
   "source": [
    "## Test on 3 classes\n",
    "\n",
    "Model predicts 4 classes\n",
    "Test data has only 3 classes\n",
    "test_ds.class_names returns the class of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055932a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"E:/data/kaggle_3/\"\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  test_path,  \n",
    "  image_size=(224, 224),\n",
    "  label_mode = 'categorical', \n",
    "  shuffle=False,\n",
    "  batch_size=32)\n",
    "\n",
    "test_labels = test_ds.class_names\n",
    "train_labels = ['Alu', 'Alu_crushed', 'Glass', 'PET']\n",
    "\n",
    "print(\"test labels: \", test_ds.class_names)\n",
    "print(\"train labels: \", train_labels)\n",
    "\n",
    "\n",
    "result = model.predict(test_ds)\n",
    "\n",
    "\n",
    "# Predictions, models returns 4 classes\n",
    "num_images, num_classes = result.shape\n",
    "predictions = []\n",
    "for i in range(num_images):    \n",
    "    predictions.append(train_labels[np.argmax(result[i])])\n",
    "\n",
    "    \n",
    "# True labels, Test data only has 3 classes    \n",
    "labels =  np.array([])\n",
    "label = []\n",
    "for _, y in test_ds:\n",
    "    labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])\n",
    "\n",
    "labels_list = labels.astype('uint8').tolist()    \n",
    "\n",
    "for i, label in enumerate(labels_list):\n",
    "    labels_list[i] = test_labels[label]\n",
    "\n",
    "print(\"unique in predictions: \", set(predictions))\n",
    "print(\"unique in true labels: \", set(labels_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3741339d",
   "metadata": {},
   "source": [
    "## Covert 4 classes pred to 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579266fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pred in enumerate(predictions):\n",
    "    if pred == \"Alu_crushed\":\n",
    "        predictions[i] = \"Alu\"\n",
    "set(predictions) # unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38413c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[250])\n",
    "print(labels_list[250])\n",
    "print(\"unique in predictions: \", set(predictions))\n",
    "print(\"unique in true labels: \", set(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4338b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "y_pred = predictions\n",
    "y_true = labels_list\n",
    "arr = confusion_matrix(y_true, y_pred)   \n",
    "# print(arr)\n",
    "# arr = confusion_matrix(correct_labels, rounded_labels)\n",
    "\n",
    "\n",
    "df_cm = pd.DataFrame(arr, index = [i for i in test_labels],\n",
    "                  columns = [i for i in test_labels])\n",
    "plt.figure(figsize=(8,6), dpi=100) # dpi for better resolution\n",
    "\n",
    "# Scale up the size of all text\n",
    "sn.set(font_scale = 1.1)\n",
    "\n",
    "# plt.ticklabel_format(useOffset=False,style='plain', axis='y')\n",
    "ax = sn.heatmap(df_cm, annot=True,fmt='d', cmap='Blues')\n",
    "ax.set_title(\"Confusion Matrix for Kaggle drinking waste\", fontsize=14, pad=20)\n",
    "plt.ylabel('Actual label')\n",
    "ax.set_ylabel(\"Actual label\", fontsize=14, labelpad=20)\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "ax.set_xlabel('Predicted label',fontsize=14, labelpad=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c829a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a0ffa",
   "metadata": {},
   "source": [
    "## Find images not accepted by tensorflow and correct their format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66876ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import imghdr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "# img_dir = test_path + \"Glass/glass2486.jpg\"\n",
    "# img_dir = r\"E:\\trashbox\\Alu\\beverage_cans256.jpg\"\n",
    "# img = mpimg.imread(img_dir)\n",
    "# print(type(img))\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "# mpimg.imsave(r\"E:\\trashbox\\Alu\\beverage_cans256.jpg\", img)\n",
    "\n",
    "\n",
    "# data_dir = test_path\n",
    "\n",
    "# test_trashbox_val_path = \"E:/data/trashbox_val/\"\n",
    "# test_trashbox_test_path = \"E:/data/trashbox_test/\"\n",
    "# test_recycling_path = \"E:/data/Recycling_test/\"\n",
    "# train_ds_path = \"E:/data/UEH_vending/UEH_vending_5class/UEH_vending_5class_train/\"\n",
    "# val_ds_path = \"E:/data/UEH_vending/UEH_vending_5class/UEH_vending_5class_val/\"\n",
    "# test_ds_path = \"E:/data/UEH_vending/UEH_vending_5class/UEH_vending_5class_val/\"\n",
    "\n",
    "train_ds_path = \"E:/data/UEH_9class/Train/\"\n",
    "val_ds_path = \"E:/data/UEH_9class/Val/\"\n",
    "\n",
    "data_dir = val_ds_path\n",
    "image_extensions = [\".png\", \".jpg\"]  # add there all your images file extensions\n",
    "\n",
    "img_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n",
    "for filepath in Path(data_dir).rglob(\"*\"):\n",
    "    if filepath.suffix.lower() in image_extensions:\n",
    "        img_type = imghdr.what(filepath)\n",
    "        if img_type is None:\n",
    "            print(f\"{filepath} is not an image\")\n",
    "            \n",
    "            # Save that image as jpg    \n",
    "            img_dir = filepath                         \n",
    "            img = mpimg.imread(img_dir)\n",
    "            mpimg.imsave(img_dir, img)                           \n",
    "        elif img_type not in img_type_accepted_by_tf:\n",
    "            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")\n",
    "            # Save that image as jpg\n",
    "            img_dir = filepath                     \n",
    "            img = mpimg.imread(img_dir)\n",
    "            mpimg.imsave(img_dir, img)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660b9da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = test_path + \"Glass/glass2486.jpg\"\n",
    "img_dir = r\"E:\\trashbox\\Alu\\beverage_cans256.jpg\"\n",
    "img = mpimg.imread(img_dir)\n",
    "print(type(img))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "mpimg.imsave(r\"E:\\trashbox\\Alu\\beverage_cans256.jpg\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264a1c4",
   "metadata": {},
   "source": [
    "# Find incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ec9d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to test\n",
    "\n",
    "test_img_real_3_path = \"E:/data/UEH_7class/Train/\"\n",
    "\n",
    "test_img_real_3 = tf.keras.utils.image_dataset_from_directory(\n",
    "  test_img_real_3_path,  \n",
    "  image_size=(224, 224),\n",
    "  label_mode = 'categorical', \n",
    "  shuffle=False,\n",
    "  batch_size=32)\n",
    "\n",
    "# Load the model\n",
    "model_name = \"InceptionV3_7class_60_epoch\"\n",
    "model = tf.keras.models.load_model('./trained_model/'+ model_name +\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_1, f1_1 = Evaluate(model, test_img_real_3_path, \"7 class train\", class_names,cm = True, normalize = \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe92529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions are from model.predict, true labels are from test_ds\n",
    "result = model.predict(test_img_real_3, verbose = True)\n",
    "\n",
    "# List of Predictions\n",
    "num_images, num_classes = result.shape\n",
    "predictions = []\n",
    "predictions_conf = []\n",
    "for i in range(num_images):\n",
    "    predictions.append(np.argmax(result[i]))\n",
    "    predictions_conf.append(round(max(result[i]), 3))\n",
    "\n",
    "# List of True labels    \n",
    "labels =  np.array([])\n",
    "label = []\n",
    "for _, y in test_img_real_3:\n",
    "    labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])\n",
    "\n",
    "labels_list = labels.astype('uint8').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535be23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_conf[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26566160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If that image is predicted incorrectly, it will be saved to the desired class output's folder.\n",
    "# The image's name will be its incorrect prediction\n",
    "\n",
    "# Load that same dataset but with batch = None\n",
    "\n",
    "test_img_no_batch = tf.keras.utils.image_dataset_from_directory(\n",
    "  test_img_real_3_path,  \n",
    "  image_size=(224, 224),\n",
    "  label_mode = 'categorical', \n",
    "  shuffle=False,\n",
    "  batch_size=None)\n",
    "\n",
    "class_names = test_img_no_batch.class_names\n",
    "    \n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "#1\n",
    "test_trashbox_val = tf.keras.utils.image_dataset_from_directory(\n",
    "  test_trashbox_val_path,  \n",
    "  image_size=(224, 224),\n",
    "  label_mode = 'categorical', \n",
    "  shuffle=False,\n",
    "  batch_size=32)\n",
    "\n",
    "class_names = test_trashbox_val.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8da3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for the class_names\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "path_incorrect = \"E:/data/INCORRECT_PRED/UEH_7class_train/\"\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    os.makedirs(path_incorrect + class_names[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If that image is predicted incorrectly, it will be saved to the desired class output's folder.\n",
    "# The image's name will be its incorrect prediction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path_incorrect = \"E:/data/INCORRECT_PRED/UEH_7class/\"\n",
    "\n",
    "i = 0\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for img, _ in test_img_no_batch:\n",
    "    if predictions[i] == labels_list[i]:\n",
    "        i += 1\n",
    "        correct += 1\n",
    "        continue\n",
    "    elif predictions[i] != labels_list[i]:        \n",
    "        # The image's name will be its incorrect prediction and confidence, saved to the desired class output's folder.\n",
    "        img_name = path_incorrect + str(class_names[labels_list[i]]) + \"/\" + str(class_names[predictions[i]]) + \"_\" + str(predictions_conf[i]) + '.jpg'\n",
    "        image = img.numpy().astype(\"uint8\")\n",
    "        plt.imsave(img_name, image)\n",
    "        i += 1\n",
    "        incorrect += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total images\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ad334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of incorrect predictions\n",
    "incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f27a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of correct predictions\n",
    "correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
